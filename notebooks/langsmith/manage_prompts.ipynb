{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage prompts programmatically\n",
    "\n",
    "## Push a prompt\n",
    "\n",
    "To create a new prompt or update an existing prompt, you can use the push prompt method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://smith.langchain.com/prompts/joke-generator/b9244726?organizationId=4791d9fe-98f1-47bb-b116-297cd74a3dc0\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "url = client.push_prompt(\"joke-generator\", object=prompt)\n",
    "# url is a link to the prompt in the UI\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub as prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://smith.langchain.com/prompts/joke-generator-2/b9244726?organizationId=4791d9fe-98f1-47bb-b116-297cd74a3dc0\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "url = prompts.push(\"joke-generator-2\", prompt)\n",
    "# url is a link to the prompt in the UI\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/joke-generator-with-model/0c1faa33?organizationId=4791d9fe-98f1-47bb-b116-297cd74a3dc0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model\n",
    "\n",
    "client.push_prompt(\"joke-generator-with-model\", object=chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull a prompt\n",
    "\n",
    "To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate.\n",
    "\n",
    "To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set).\n",
    "\n",
    "To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_818c284075', 'finish_reason': 'stop', 'logprobs': None}, id='run-611791d6-dc2b-402f-ba20-644df778112c-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = client.pull_prompt(\"joke-generator\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-98ef5997-a302-4a73-95d6-312c2a96a10d-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompts.pull(\"joke-generator\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the cat sit on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'stop', 'logprobs': None}, id='run-ac1e992f-8adf-4bc5-9df0-165d5fde6361-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)\n",
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When pulling a prompt, you can also specify a specific commit hash or prompt tag to pull a specific version of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = client.pull_prompt(\"joke-generator:b9244726\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = client.pull_prompt(\"efriis/my-first-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['profession'], input_types={}, partial_variables={}, template='You are an expert {profession} who loves answering questions cheerfully.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question} '), additional_kwargs={})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a prompt without LangChain\n",
    "\n",
    "If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API.\n",
    "\n",
    "These conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai client\n",
    "oai_client = OpenAI()\n",
    "\n",
    "# pull prompt and invoke to populate the variables\n",
    "prompt = client.pull_prompt(\"joke-generator\")\n",
    "prompt_value = prompt.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "openai_payload = convert_prompt_to_openai_format(prompt_value)\n",
    "openai_response = oai_client.chat.completions.create(**openai_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Ab60TUYMQic8YbdJi5MnmQgVu7mel', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733404589, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=20, prompt_tokens=13, total_tokens=33, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from langsmith.client import convert_prompt_to_anthropic_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anthropic client\n",
    "anthropic_client = Anthropic()\n",
    "# pull prompt and invoke to populate the variables\n",
    "prompt = client.pull_prompt(\"joke-generator\")\n",
    "prompt_value = prompt.invoke({\"topic\": \"cats\"})\n",
    "anthropic_payload = convert_prompt_to_anthropic_format(prompt_value)\n",
    "anthropic_response = anthropic_client.messages.create(**anthropic_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01UR3mgaqvrVWF4x29dcZUBR', content=[TextBlock(text='Here\\'s a cat joke for you:\\n\\nWhy don\\'t cats play poker in the jungle? \\nToo many cheetahs!\\n\\nHow\\'s that? Hopefully you got a little chuckle out of the pun on \"cheetahs\" sounding like \"cheaters.\" Cats and their love of puns - or at least our human attempts at cat-themed puns. Let me know if you\\'d like to hear another cat joke.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=13, output_tokens=99))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anthropic_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List, delete, and like prompts\n",
    "\n",
    "You can also list, delete, and like/unlike prompts using the list prompts, delete prompt, like prompt and unlike prompt methods. See the LangSmith SDK client for extensive documentation on these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all prompts in my workspace\n",
    "prompts = client.list_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List my private prompts that include \"joke\"\n",
    "prompts = client.list_prompts(query=\"joke\", is_public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListPromptsResponse(repos=[Prompt(repo_handle='joke-generator-with-model', description='', readme='', id='845f5157-b351-46ca-a8d5-4f890a617b46', tenant_id='4791d9fe-98f1-47bb-b116-297cd74a3dc0', created_at=datetime.datetime(2024, 12, 5, 13, 10, 7, 53907), updated_at=datetime.datetime(2024, 12, 5, 13, 10, 8, 186709), is_public=False, is_archived=False, tags=['ChatPromptTemplate'], original_repo_id=None, upstream_repo_id=None, owner=None, full_name='joke-generator-with-model', num_likes=0, num_downloads=1, num_views=0, liked_by_auth_user=False, last_commit_hash='0c1faa33da1f14c5669245781eaca359d34a515c6054b9f82252719e87ea74b5', num_commits=1, original_repo_full_name=None, upstream_repo_full_name=None), Prompt(repo_handle='joke-generator-2', description='', readme='', id='557f075e-aef5-4b14-be13-d7b45830f7c8', tenant_id='4791d9fe-98f1-47bb-b116-297cd74a3dc0', created_at=datetime.datetime(2024, 12, 5, 13, 9, 9, 144338), updated_at=datetime.datetime(2024, 12, 5, 13, 9, 11, 269719), is_public=False, is_archived=False, tags=['ChatPromptTemplate'], original_repo_id=None, upstream_repo_id=None, owner=None, full_name='joke-generator-2', num_likes=0, num_downloads=0, num_views=0, liked_by_auth_user=False, last_commit_hash='b924472604115f2f6b4c43099125a6dc97080f6cefc846c0749bdab24b933f71', num_commits=1, original_repo_full_name=None, upstream_repo_full_name=None), Prompt(repo_handle='joke-generator', description='', readme='', id='a74623ac-e7d1-4504-8a66-db64beb17ce4', tenant_id='4791d9fe-98f1-47bb-b116-297cd74a3dc0', created_at=datetime.datetime(2024, 12, 5, 13, 8, 3, 116901), updated_at=datetime.datetime(2024, 12, 5, 13, 8, 53, 71813), is_public=False, is_archived=False, tags=['ChatPromptTemplate'], original_repo_id=None, upstream_repo_id=None, owner=None, full_name='joke-generator', num_likes=0, num_downloads=5, num_views=1, liked_by_auth_user=False, last_commit_hash='b924472604115f2f6b4c43099125a6dc97080f6cefc846c0749bdab24b933f71', num_commits=1, original_repo_full_name=None, upstream_repo_full_name=None)], total=3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a prompt\n",
    "client.delete_prompt(\"joke-generator-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'likes': 1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like a prompt\n",
    "client.like_prompt(\"efriis/my-first-prompt\")\n",
    "# Unlike a prompt\n",
    "client.unlike_prompt(\"efriis/my-first-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
