{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your LLM application\n",
    "\n",
    "It can be hard to measure the performance of your application with respect to criteria important you or your users. However, doing so is crucial, especially as you iterate on your application. We will use LangSmith to measure how well your application is performing over a \n",
    "fixed set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.\n",
    "\n",
    "At a high level, in this tutorial we will go over how to:\n",
    "\n",
    "- Create an initial golden dataset to measure performance\n",
    "- Define metrics to use to measure performance\n",
    "- Run evaluations on a few different prompts or models\n",
    "- Compare results manually\n",
    "- Track results over time\n",
    "- Set up automated testing to run in CI/CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\n",
    "\n",
    "- What should the schema of each datapoint be?\n",
    "- How many datapoints should I gather?\n",
    "- How should I gather those datapoints?\n",
    "\n",
    "**Schema**: Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.\n",
    "\n",
    "**How many**: There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!\n",
    "\n",
    "**How to get**: This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally living constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling ~10-20 examples.\n",
    "\n",
    "Once you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).\n",
    "\n",
    "We will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"QA Example Dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"What is LangChain?\"},\n",
    "        {\"question\": \"What is LangSmith?\"},\n",
    "        {\"question\": \"What is OpenAI?\"},\n",
    "        {\"question\": \"What is Google?\"},\n",
    "        {\"question\": \"What is Mistral?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"answer\": \"A framework for building LLM applications\"},\n",
    "        {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        {\"answer\": \"A technology company known for search\"},\n",
    "        {\"answer\": \"A company that creates Large Language Models\"},\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page, when we click into it we should see that we have five new examples.\n",
    "\n",
    "![LangSmith Examples](../../assets/langsmith_examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics\n",
    "\n",
    "After creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier.\n",
    "\n",
    "In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.\n",
    "\n",
    "Let's go ahead and define these two metrics.\n",
    "\n",
    "For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This **LLM-as-a-judge** is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langsmith.evaluation import LangChainStringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "\n",
    "{query}\n",
    "\n",
    "Here is the real answer:\n",
    "\n",
    "{answer}\n",
    "\n",
    "You are grading the following predicted answer:\n",
    "\n",
    "{result}\n",
    "\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0.0)\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "def evaluate_length(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    required = example.outputs.get(\"answer\") or \"\"\n",
    "    score = int(len(prediction) < 2 * len(required))\n",
    "    return {\"key\":\"length\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations\n",
    "\n",
    "Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "openai_client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_app(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langsmith_app(inputs):\n",
    "    output = my_app(inputs[\"question\"])\n",
    "    return {\"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'openai-3.5-69fa581d' at:\n",
      "https://smith.langchain.com/o/4791d9fe-98f1-47bb-b116-297cd74a3dc0/datasets/957bd0d2-fb1b-49b9-a67e-2908acfc7bdb/compare?selectedSessions=fe055b5f-597a-452c-bdfc-52ac3533a5f3\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f92d06898a24550b5f9b7048b06e738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app,                              # Your AI system\n",
    "    data=dataset_name,                          # The data to predict and grade over\n",
    "    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    experiment_prefix=\"openai-3.5\",             # A prefix for your experiment names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangSmith Experiments](../../assets/langsmith_experiments.png)\n",
    "\n",
    "If we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run!\n",
    "\n",
    "![LangSmith Experiments Tab](../../assets/langsmith_experiments_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with a different model\n",
    "\n",
    "Let's try gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_app_1(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langsmith_app_1(inputs):\n",
    "    output = my_app_1(inputs[\"question\"])\n",
    "    return {\"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'openai-4-24ce7466' at:\n",
      "https://smith.langchain.com/o/4791d9fe-98f1-47bb-b116-297cd74a3dc0/datasets/957bd0d2-fb1b-49b9-a67e-2908acfc7bdb/compare?selectedSessions=3e99b53b-2445-4946-aad1-6b324100f56d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f6fe170f20426d9857584c5e809bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    langsmith_app_1,\n",
    "    data=dataset_name,\n",
    "    evaluators=[evaluate_length, qa_evaluator],\n",
    "    experiment_prefix=\"openai-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_app_2(question):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content\n",
    "\n",
    "\n",
    "def langsmith_app_2(inputs):\n",
    "    output = my_app_2(inputs[\"question\"])\n",
    "    return {\"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'openai-4-71c6e0df' at:\n",
      "https://smith.langchain.com/o/4791d9fe-98f1-47bb-b116-297cd74a3dc0/datasets/957bd0d2-fb1b-49b9-a67e-2908acfc7bdb/compare?selectedSessions=b06d5818-51db-4240-b268-3bf9b6b6d356\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7a19fff6fe433285340d3d67e96698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    langsmith_app_2,\n",
    "    data=dataset_name,\n",
    "    evaluators=[evaluate_length, qa_evaluator],\n",
    "    experiment_prefix=\"openai-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Now we can compare the results of the three different runs. In the experiemnts tab we can see the correctness and length metrics for each run.\n",
    "\n",
    "![LangSmith Experiments Compare](../../assets/langsmith_experiments_compare.png)\n",
    "\n",
    "So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length. But what if we want to explore in more detail?\n",
    "\n",
    "In order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view:\n",
    "\n",
    "![LangSmith Compare Experiments](../../assets/langsmith_compare_experiments.png)\n",
    "\n",
    "We immediately see all three tests side by side. Some of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline. We automatically choose defaults for the baseline and metric, but you can change those yourself (outlined in blue below). You can also choose which columns and which metrics you see by using the Display control (outlined in yellow below). You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top (outlined in red below).\n",
    "\n",
    "If we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up automated testing to run in CI/CD\n",
    "\n",
    "Now that we've run this in a one-off manner, we can set it to run in an automated fashion. We can do this pretty easily by just including it as a pytest file that we run in CI/CD. As part of this, we can either just log the results OR set up some criteria to determine if it passes or not. For example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check, we could set that up with a test like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_length_score() -> None:\n",
    "    \"\"\"Test that the length score is at least 80%.\"\"\"\n",
    "    experiment_results = evaluate(\n",
    "        langsmith_app, # Your AI system\n",
    "        data=dataset_name, # The data to predict and grade over\n",
    "        evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    )\n",
    "    # This will be cleaned up in the next release:\n",
    "    feedback = client.list_feedback(\n",
    "        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],\n",
    "        feedback_key=\"length\"\n",
    "    )\n",
    "    scores = [f.score for f in feedback]\n",
    "    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track results over time\n",
    "\n",
    "Now that we've got these experiments running in an automated fashion, we want to track these results over time. We can do this from the overall Experiments tab in the datasets page. By default, we show evaluation metrics over time (highlighted in red). We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).\n",
    "\n",
    "![LangSmith Git Tracking](../../assets/git_tracking.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
