{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run pairwise evaluations\n",
    "\n",
    "LangSmith supports evaluating existing experiments in a comparative manner. This allows you to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. To do this, use the `evaluate_comparative` function with two existing experiments.\n",
    "\n",
    "## `evaluate_comparative` args\n",
    "\n",
    "- `experiments`: A list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.\n",
    "- `evaluators`: A list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how\n",
    "\n",
    "Optional args:\n",
    "\n",
    "- `randomize_order`: An optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.\n",
    "- `experiment_prefix`: A prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.\n",
    "- `description`: A description of the pairwise experiment. Defaults to None.\n",
    "- `max_concurrency`: The maximum number of concurrent evaluations to run. Defaults to 5.\n",
    "- `client`: The LangSmith client to use. Defaults to None.\n",
    "- `metadata`: Metadata to attach to your pairwise experiment. Defaults to None.\n",
    "- `load_nested`: Whether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False.\n",
    "\n",
    "## Run a pairwise evaluation\n",
    "\n",
    "The following example uses a prompt which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langsmith.evaluation import evaluate_comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")\n",
    "model = init_chat_model(\"gpt-4o\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n",
    "    response = chain.invoke({\n",
    "        \"question\": inputs[\"question\"],\n",
    "        \"answer_a\": outputs[0].get(\"output\", \"N/A\"),\n",
    "        \"answer_b\": outputs[1].get(\"output\", \"N/A\"),\n",
    "    })\n",
    "    preference = response[\"Preference\"]\n",
    "\n",
    "    if preference == 1:\n",
    "        scores = [1, 0]\n",
    "    elif preference == 2:\n",
    "        scores = [0, 1]\n",
    "    else:\n",
    "        scores = [0, 0]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/4791d9fe-98f1-47bb-b116-297cd74a3dc0/datasets/957bd0d2-fb1b-49b9-a67e-2908acfc7bdb/compare?selectedSessions=b06d5818-51db-4240-b268-3bf9b6b6d356%2C3e99b53b-2445-4946-aad1-6b324100f56d&comparativeExperiment=36106bf2-9e8a-4687-8eaf-e3d89a8f2c25\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217d2389f1c24a50972093fdbccc224c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<langsmith.evaluation._runner.ComparativeExperimentResults at 0x10a416f90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_comparative(\n",
    "    # Replace the following array with the names or IDs of your experiments\n",
    "    [\"openai-4-71c6e0df\", \"openai-4-24ce7466\"],\n",
    "    evaluators=[ranked_preference],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pairwise experiments](../../assets/pairwise_experiments.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
