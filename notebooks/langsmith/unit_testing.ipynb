{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to unit test applications\n",
    "\n",
    "LangSmith functional tests are assertions and expectations designed to quickly identify obvious bugs and regressions in your AI system. Relative to evaluations, tests typically are designed to be fast and cheap to run, focusing on specific functionality and edge cases. You can use LangSmith to track any unit tests, end-to-end integration tests, or other specific assertions that touch an LLM or other non-deterministic part of your AI system. These should run on every commit in your CI pipeline to catch regressions early.\n",
    "\n",
    "## Write a `@unit`\n",
    "\n",
    "To write a LangSmith functional test, decorate your test function with @unit. If you want to track the full nested trace of the system or component being tested, you can mark those functions with @traceable. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_app/main.py\n",
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "@traceable # Optional\n",
    "def generate_sql(user_query):\n",
    "    # Replace with your SQL generation logic\n",
    "    # e.g., my_llm(my_prompt.format(user_query))\n",
    "    return \"SELECT * FROM customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_my_app.py\n",
    "from langsmith import unit\n",
    "# from my_app.main import generate_sql\n",
    "\n",
    "\n",
    "@unit\n",
    "def test_sql_generation_select_all():\n",
    "    user_query = \"Get all users from the customers table\"\n",
    "    sql = generate_sql(user_query)\n",
    "    # LangSmith logs any exception raised by `assert` / `pytest.fail` / `raise` / etc.\n",
    "    # as a test failure\n",
    "    assert sql == \"SELECT * FROM customers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests\n",
    "\n",
    "You can use a standard testing framework such as pytest (docs) to run. For example:\n",
    "\n",
    "```bash\n",
    "pytest tests/\n",
    "```\n",
    "\n",
    "Each time you run this test suite, LangSmith collects the pass/fail rate and other traces as a new TestSuiteResult, logging the pass rate (1 for pass, 0 for fail) over all the applicable tests.\n",
    "\n",
    "The test suite syncs to a corresponding dataset named after your package or github repository.\n",
    "\n",
    "![Unit test suite](../../assets/unit_test_suite.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "`@unit` is designed to stay out of your way and works well with familiar pytest features. For example:\n",
    "\n",
    "**Defining inputs as fixtures**\n",
    "\n",
    "Pytest fixtures let you define functions that serve as reusable inputs for your tests. LangSmith automatically syncs any test case inputs defined as fixtures. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def user_query():\n",
    "    return \"Get all users from the customers table\"\n",
    "\n",
    "@pytest.fixture\n",
    "def expected_sql():\n",
    "    return \"SELECT * FROM customers\"\n",
    "\n",
    "# output_keys indicate which test arguments to save as 'outputs' in the dataset (Optional)\n",
    "# Otherwise, all arguments are saved as 'inputs'\n",
    "@unit(output_keys=[\"expected_sql\"])\n",
    "def test_sql_generation_with_fixture(user_query, expected_sql):\n",
    "    sql = generate_sql(user_query)\n",
    "    assert sql == expected_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametrizing tests**\n",
    "\n",
    "Parametrizing tests lets you run the same assertions across multiple sets of inputs. Use pytest's parametrize decorator to achieve this. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unit(output_keys=[\"expected_sql\"])\n",
    "@pytest.mark.parametrize(\n",
    "    \"user_query, expected_sql\",\n",
    "    [\n",
    "        (\"Get all users from the customers table\", \"SELECT * FROM customers\"),\n",
    "        (\"Get all users from the orders table\", \"SELECT * FROM orders\"),\n",
    "    ],\n",
    ")\n",
    "def test_sql_generation_parametrized(user_query, expected_sql):\n",
    "    sql = generate_sql(user_query)\n",
    "    assert sql == expected_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: as the parametrized list grows, you may consider using evaluate() instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.\n",
    "\n",
    "**Expectations**\n",
    "\n",
    "LangSmith provides an expect utility to help define expectations about your LLM output. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import expect\n",
    "\n",
    "\n",
    "@unit\n",
    "def test_sql_generation_select_all():\n",
    "    user_query = \"Get all users from the customers table\"\n",
    "    sql = generate_sql(user_query)\n",
    "    expect(sql).to_contain(\"customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will log the binary \"expectation\" score to the experiment results, additionally asserting that the expectation is met possibly triggering a test failure.\n",
    "\n",
    "expect also provides \"fuzzy match\" methods. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_chatbot(query):\n",
    "    return \"Paris\"\n",
    "\n",
    "@unit(output_keys=[\"expectation\"])\n",
    "@pytest.mark.parametrize(\n",
    "    \"query, expectation\",\n",
    "    [\n",
    "       (\"what's the capital of France?\", \"Paris\"),\n",
    "    ],\n",
    ")\n",
    "def test_embedding_similarity(query, expectation):\n",
    "    prediction = my_chatbot(query)\n",
    "    expect.embedding_distance(\n",
    "        # This step logs the distance as feedback for this run\n",
    "        prediction=prediction, expectation=expectation\n",
    "    # Adding a matcher (in this case, 'to_be_*\"), logs 'expectation' feedback\n",
    "    ).to_be_less_than(0.5) # Optional predicate to assert against\n",
    "    expect.edit_distance(\n",
    "        # This computes the normalized Damerau-Levenshtein distance between the two strings\n",
    "        prediction=prediction, expectation=expectation\n",
    "    # If no predicate is provided below, 'assert' isn't called, but the score is still logged\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test case will be assigned 4 scores:\n",
    "\n",
    "- The `embedding_distance` between the prediction and the expectation\n",
    "- The binary expectation score (1 if cosine distance is less than 0.5, 0 if not)\n",
    "- The `edit_distance` between the prediction and the expectation\n",
    "- The overall test pass/fail score (binary)\n",
    "\n",
    "The expect utility is modeled off of Jest's expect API, with some off-the-shelf functionality to make it easier to grade your LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dry-run mode**\n",
    "\n",
    "If you want to run the tests without syncing the results to LangSmith, you can set `LANGCHAIN_TEST_TRACKING=false` in your environment.\n",
    "\n",
    "```bash\n",
    "LANGCHAIN_TEST_TRACKING=false pytest tests/\n",
    "```\n",
    "\n",
    "The tests will run as normal, but the experiment logs will not be sent to LangSmith.\n",
    "\n",
    "**Caching**\n",
    "\n",
    "LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache results to disk. Any identical inputs will be loaded from the cache so you don't have to call out to your LLM provider unless there are changes to the model, prompt, or retrieved data.\n",
    "\n",
    "To enable caching, run with `LANGCHAIN_TEST_CACHE=/my/cache/path`. For example:\n",
    "\n",
    "```bash\n",
    "LANGCHAIN_TEST_CACHE=tests/cassettes pytest tests/my_llm_tests\n",
    "```\n",
    "\n",
    "All requests will be cached to tests/cassettes and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well.\n",
    "\n",
    "**Using watch mode**\n",
    "\n",
    "With caching enabled, you can iterate quickly on your tests using watch mode without worrying about unnecessarily hitting your LLM provider. For example, using `pytest-watch`:\n",
    "\n",
    "```bash\n",
    "pip install pytest-watch\n",
    "LANGCHAIN_TEST_CACHE=tests/cassettes ptw tests/my_llm_tests\n",
    "```\n",
    "\n",
    "## Explanations\n",
    "\n",
    "The `@unit` test decorator converts any test into a parametrized LangSmith example. By default, all tests within a given file will be grouped as a single \"test suite\" with a corresponding dataset. You can configure which test suite a test belongs to by passing the `test_suite_name` parameter to `@unit`.\n",
    "\n",
    "The following metrics are available off-the-shelf:\n",
    "\n",
    "| Feedback             | Description                                                 | Example                                                                                                               |\n",
    "| -------------------- | ----------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| `pass`               | Binary pass/fail score, 1 for pass, 0 for fail              | `assert False` # Fails                                                                                                |\n",
    "| `expectation`        | Binary expectation score, 1 if expectation is met, 0 if not | `expect(prediction).against(lambda x: re.search(r\"\\b[a-f\\d]{8}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{12}\\b\", x)` ) |\n",
    "| `embedding_distance` | Cosine distance between two embeddings                      | expect.embedding_distance(prediction=prediction, expectation=expectation)                                             |\n",
    "| `edit_distance`      | Edit distance between two strings                           | expect.edit_distance(prediction=prediction, expectation=expectation)                                                  |\n",
    "\n",
    "You can also log any arbitrary feedback within a unit test manually using the `client`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client, unit\n",
    "from langsmith.run_helpers import get_current_run_tree\n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "@unit\n",
    "def test_foo():\n",
    "    run_tree = get_current_run_tree()\n",
    "    client.create_feedback(run_id=run_tree.id, key=\"my_custom_feedback\", score=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `test` API\n",
    "\n",
    "The `@unit` decorator is used to mark a function as a test case for LangSmith. It ensures that the necessary example data is created and associated with the test function. The decorated function will be executed as a test case, and the results will be recorded and reported by LangSmith.\n",
    "\n",
    "```python\n",
    "@unit(id=None, output_keys=None, client=None, test_suite_name=None)\n",
    "```\n",
    "\n",
    "Create a test case in LangSmith.\n",
    "\n",
    "Parameters\n",
    "\n",
    "- `id` (`Optional[uuid.UUID]`): A unique identifier for the test case. If not provided, an ID will be generated based on the test function's module and name.\n",
    "- `output_keys` (`Optional[Sequence[str]]`): A list of keys to be considered as the output keys for the test case. These keys will be extracted from the test function's inputs and stored as the expected outputs.\n",
    "- `client` (`Optional[ls_client.Client]`): An instance of the LangSmith client to be used for communication with the LangSmith service. If not provided, a default client will be used.\n",
    "- `test_suite_name` (`Optional[str]`): The name of the test suite to which the test case belongs. If not provided, the test suite name will be determined based on the environment or the package name.\n",
    "\n",
    "Environment Variables\n",
    "\n",
    "- `LANGSMITH_TEST_TRACKING` (`Optional[str]`): Set this variable to the path of a directory to enable caching of test results. This is useful for re-running tests without re-executing the code. Requires the `langsmith[vcr]` package.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
