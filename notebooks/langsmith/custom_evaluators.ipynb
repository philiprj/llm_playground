{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to define custom evaluators\n",
    "\n",
    "Custom evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into evaluate() / aevaluate().\n",
    "\n",
    "## Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"answer\"] == reference_outputs[\"answer\"]\n",
    "\n",
    "def dummy_app(inputs: dict) -> dict:\n",
    "    return {\"answer\": \"hmm i'm not sure\", \"reasoning\": \"i didn't understand the question\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(\n",
    "    dummy_app,\n",
    "    data=\"dataset_name\",    # Update with real dataset name\n",
    "    evaluators=[correct]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Args\n",
    "\n",
    "Custom evaluator functions must have specific argument names. They can take any subset of the following arguments:\n",
    "\n",
    "- `run`: `langsmith.schemas.Run`: The full Run object generated by the application on the given example.\n",
    "- `example`: `langsmith.schemas.Example`: The full dataset Example, including the example inputs, outputs, and metadata.\n",
    "- `inputs`: `dict`: A dictionary of the inputs corresponding to a single example in a dataset.\n",
    "- `outputs`: `dict`: A dictionary of the outputs generated by the application on the given inputs.\n",
    "- `reference_outputs`: `dict`:A dictionary of the reference outputs associated with the example, if available.\n",
    "\n",
    "For most use cases you'll only need inputs, outputs, and reference_outputs. run and example are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.\n",
    "\n",
    "## Evaluator Output\n",
    "\n",
    "Custom evaluators are expected to return one of the following types:\n",
    "\n",
    "- `dict`:  dicts of the form `{\"score\" | \"value\": ..., \"name\": ...}` allow you to customize the metric type (\"score\" for numerical and \"value\" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.\n",
    "- `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.\n",
    "- `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.\n",
    "- `list[dict]`: return multiple metrics using a single function.\n",
    "\n",
    "## Additional examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import evaluate, wrappers\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare actual and reference outputs\n",
    "def correct(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"answer\"] == reference_outputs[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just evaluate actual outputs\n",
    "def concision(outputs: dict) -> int:\n",
    "    \"\"\"Score how concise the answer is. 1 is the most concise, 5 is the least concise.\"\"\"\n",
    "    return min(len(outputs[\"answer\"]) // 1000, 4) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an LLM-as-a-judge\n",
    "oai_client = wrappers.wrap_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def valid_reasoning(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"Use an LLM to judge if the reasoning and the answer are consistent.\"\"\"\n",
    "\n",
    "    instructions = \"\"\"\\\n",
    "\n",
    "Given the following question, answer, and reasoning, determine if the reasoning for the \\\n",
    "answer is logically valid and consistent with question and the answer.\"\"\"\n",
    "\n",
    "    class Response(BaseModel):\n",
    "        reasoning_is_valid: bool\n",
    "\n",
    "    msg = f\"Question: {inputs['question']}\\nAnswer: {outputs['answer']}\\nReasoning: {outputs['reasoning']}\"\n",
    "    response = await oai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": instructions,}, {\"role\": \"user\", \"content\": msg}],\n",
    "        response_format=Response\n",
    "    )\n",
    "    return response.choices[0].message.parsed.reasoning_is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_app(inputs: dict) -> dict:\n",
    "    return {\"answer\": \"hmm i'm not sure\", \"reasoning\": \"i didn't understand the question\"}\n",
    "\n",
    "# results = evaluate(\n",
    "#     dummy_app,\n",
    "#     data=\"dataset_name\",\n",
    "#     evaluators=[correct, concision, valid_reasoning]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
